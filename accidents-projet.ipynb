{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # dataframe structure\n",
    "import numpy as np  # array structure\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt # plot\n",
    "import glob, os  # dealing with OS and Files, reading files etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>A noter j'ai supprimé les données concernant 2009 car n'a pas la même forme que les autres ( juste pour lire tout en même temps)</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier on va lire tous les fichiers csv qu'on a téléchargé à partir du site datagov :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data set about caracteristiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/caracteristiques' # path containing all csv files about caracteristiques (  use your own path)\n",
    "allFiles = glob.glob(path + \"/*.csv\")  # a list containing files names (all csv files)\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1') # read csv file (file_)\n",
    "    list_.append(df)  # append the the dataframe df to the lust list_\n",
    "caracteristiques = pd.concat(list_)  # concat all dataframes existing in list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Data set about usagers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/usagers' \n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1')\n",
    "    list_.append(df)\n",
    "usagers = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data set about lieux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/lieux' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1')\n",
    "    list_.append(df)\n",
    "lieux = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set about vehicules :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/vehicules' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1')\n",
    "    list_.append(df)\n",
    "vehicules = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Short description about our dataframes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"==== Shape of our dataframes ====\")\n",
    "print(\"The shape of caracteristiques is :\", caracteristiques.shape)\n",
    "print(\"The shape of lieux is :\", lieux.shape)\n",
    "print(\"The shape of vehicules is :\", vehicules.shape)\n",
    "print(\"The shape of usagers is :\", usagers.shape)\n",
    "print(\"==== Count Missing Data (note that 0 means also a nan value but should be converted to nan later during cleaning stage)  ====\")\n",
    "print(\"Total missing data in  caracteristiques is :\", caracteristiques.isnull().sum().sum())\n",
    "print(\"Total missing data in  lieux is :\", lieux.isnull().sum().sum())\n",
    "print(\"Total missing data in  vehicules is :\", vehicules.isnull().sum().sum())\n",
    "print(\"Total missing data in  usagers is :\", usagers.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche 1 : Prédiction de la gravité des accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons essayer d'effectuer un apprentissage supervisé sur la variable : gravité de l'accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne prenons pas en compte les variables spatiales. Donc nous ne faisons pas de nettoyage pour ces variables pour cette première tâche, mais nous allons nettoyer les autres variables ( imputation des valeurs manquantes, les points abberrants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va nettoyer les quatre tables qu'on vient de récupérer ci-dessus. Chaque table contient l'historique de 2005 jusqu'à 2016 ( sauf 2009).\n",
    "Une fois les tables sont bien nettoyées, on va effectuer une jointure afin d'avoir toutes les variables nécessaires pour effectuer notre classification. La table résultante sera à la fois notre jeu de données sur toute la période(2005-2016) et  contenant les sous jeu de données correspondants à chaque année.\n",
    "Avant de commencer la modélisation, nous allons effectuer des explorations ainsi que des analyses statistiques visant d'une part à visualiser les données et d'autre part à comprendre le jeu de donnée. Cette étape est primordiale car il constitue une étape de la méthode CRISP -largement utilisée dans le monde industrielle- utilisée en data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning  :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "####  La table caracteristiques \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "# ici on prend en compte que les nan\n",
    "msno.matrix(caracteristiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### gps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(caracteristiques.gps.unique()))\n",
    "caracteristiques.gps.unique()\n",
    "#On voit que gps a 8 valeurs uniques, alors que dans le documents pdf il n'y a que 5 ( M,A,G,R et Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nan, 0 et S se sont considérées comme des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques[caracteristiques.gps=='S'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques[caracteristiques.gps=='0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.gps.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# puisque cette variable est utilisée pour localiser l'accident, on va remplacer '0' et'S' par nan.\n",
    "caracteristiques.loc[:,['gps']]=caracteristiques.loc[:,['gps']].replace('0',np.nan)\n",
    "caracteristiques.loc[:,['gps']]=caracteristiques.loc[:,['gps']].replace('S',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verification :\n",
    "print(len(caracteristiques.gps.unique()))\n",
    "caracteristiques.gps.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.gps=caracteristiques.gps.astype(str) # on change ty type car nan est float alors que la variable est un object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques[caracteristiques.gps=='nan'].head() # pour voir le lien entre gps et dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques[caracteristiques.gps=='nan'].tail() # pour voir le lien ente gps et dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer que il y a des nan dans la france metropole et la france d'outre mer.\n",
    "\n",
    "-Donc pour France Metropole on remplace gps par M ( il suffit de voir dep<950)\n",
    "\n",
    "-Donc pour la france d'outre mer il suffit de voir aussi dep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# France M :\n",
    "caracteristiques.ix[caracteristiques.dep<=950,'gps']=caracteristiques.ix[caracteristiques.dep<=950,'gps'].replace('nan','M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.ix[caracteristiques.dep==971,'gps']=caracteristiques.ix[caracteristiques.dep==971,'gps'].replace('nan','A')\n",
    "caracteristiques.ix[caracteristiques.dep==972,'gps']=caracteristiques.ix[caracteristiques.dep==972,'gps'].replace('nan','A')\n",
    "caracteristiques.ix[caracteristiques.dep==973,'gps']=caracteristiques.ix[caracteristiques.dep==973,'gps'].replace('nan','G')\n",
    "caracteristiques.ix[caracteristiques.dep==974,'gps']=caracteristiques.ix[caracteristiques.dep==974,'gps'].replace('nan','R')\n",
    "caracteristiques.ix[caracteristiques.dep==976,'gps']=caracteristiques.ix[caracteristiques.dep==976,'gps'].replace('nan','Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc cette variable est bien imputée.\n",
    "Il reste juste le fait de fixer le type de cette variable. Elle est déjà 'object'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### jour :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.jour.unique()))\n",
    "caracteristiques.jour.unique()\n",
    "# pas de soucis pour cette variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### mois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.mois.unique()))\n",
    "caracteristiques.mois.unique()\n",
    "# pas de soucis pour cette variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### an :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.an.unique()))\n",
    "caracteristiques.an.unique()\n",
    "#pareil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### hrmn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.hrmn.min(), caracteristiques.hrmn.max()\n",
    "# donc ici il s'agit de l'heure et les minutes\n",
    "# exemple : 2359 ==> 23:59\n",
    "# on cherche le len des éléments \n",
    "mes_len=[len(ele) for ele in set(list(caracteristiques.hrmn.astype(str).values))]\n",
    "[ele for ele in set(mes_len)]\n",
    "# Il y a donc différents len ( de 1 ==>4) \n",
    "# donc if faut du traitement\n",
    "# si len == 4 ==> ajouter : entre les deux chiffres\n",
    "# si len == 3 ==> ajouter 0 en avant puis : entre les deux chiffres\n",
    "# si len == 2 ==> ajouter deux 00 vers la fin puis : \n",
    "# si len == 1 ==> ajouter 0 avant et 00 à la fin puis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change type to str for easy handling :\n",
    "caracteristiques.hrmn=caracteristiques.hrmn.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len ==4 : split with len=2 then add :\n",
    "caracteristiques.ix[caracteristiques.hrmn.str.len()==4,'hrmn']= caracteristiques.ix[caracteristiques.hrmn.str.len()==4,'hrmn'].str.extract('(.{2,2})' * 2).apply(lambda x: ':'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len == 3 :  add 0 to into the start then split then add :\n",
    "caracteristiques.ix[caracteristiques.hrmn.str.len()==3,'hrmn']=('0'+caracteristiques.ix[caracteristiques.hrmn.str.len()==3,'hrmn']).str.extract('(.{2,2})' * 2).apply(lambda x: ':'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len =2 : add 00 to the end then split then add : \n",
    "caracteristiques.ix[caracteristiques.hrmn.str.len()==2,'hrmn']=(caracteristiques.ix[caracteristiques.hrmn.str.len()==2,'hrmn'] +'00').str.extract('(.{2,2})' * 2).apply(lambda x: ':'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len == 1 ==> ajouter 0 avant et 00 à la fin puis : \n",
    "caracteristiques.ix[caracteristiques.hrmn.str.len()==1,'hrmn']=('0'+caracteristiques.ix[caracteristiques.hrmn.str.len()==1,'hrmn'] +'00').str.extract('(.{2,2})' * 2).apply(lambda x: ':'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# verification :\n",
    "mes_len_apres=[len(ele) for ele in set(list(caracteristiques.hrmn.astype(str).values))]\n",
    "[ele for ele in set(mes_len_apres)]\n",
    "# c'est bon ( len==5 car on a ajouté les :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### lum :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.lum.unique()))\n",
    "caracteristiques.lum.unique()\n",
    "# bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change type to object\n",
    "caracteristiques.lum=caracteristiques.lum.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques['agg'].unique()))\n",
    "caracteristiques['agg'].unique()\n",
    "#bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.loc[:,['agg']]=caracteristiques.loc[:,['agg']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### intersection :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.int.unique()))\n",
    "caracteristiques.int.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 est un intrus \n",
    "caracteristiques[caracteristiques.int==0].shape\n",
    "# il y 106 valeurs de 0\n",
    "# on va remplacer cette valeur par la valeur la plus courante "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.int.astype(str).describe()\n",
    "# le plus courant c'est 1 avec une fréquence de 547440\n",
    "# c'est normale car la majorité des accidents n'ont pas lieu sur les intersections ( vitesse très faible générelement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remplacer 0 par 1: \n",
    "caracteristiques.loc[:,['int']]=caracteristiques.loc[:,['int']].replace(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verifications :\n",
    "print(len(caracteristiques.int.unique()))\n",
    "caracteristiques.int.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.int=caracteristiques.int.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### atm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.atm.unique()))\n",
    "caracteristiques.atm.unique()\n",
    "# il y des nan values, de même on va les remplacer par la valeur la plus courante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taille des nan values dans atm :\n",
    "print('taille :', caracteristiques.atm.isnull().sum())\n",
    "# valeur la plus courante dans atm :\n",
    "caracteristiques.atm.astype(str).describe()\n",
    "# 1 est la valeur la plus courante ( normale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remplacer nan par 1 :\n",
    "caracteristiques.loc[:,['atm']]=caracteristiques.loc[:,['atm']].replace(np.nan,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verification :\n",
    "print(len(caracteristiques.atm.unique()))\n",
    "caracteristiques.atm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.atm=caracteristiques.atm.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### collision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.col.unique()))\n",
    "caracteristiques.col.unique()\n",
    "# nan values are in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taille des nan values dans col :\n",
    "print('taille :', caracteristiques.col.isnull().sum())\n",
    "# valeur la plus courante dans col :\n",
    "caracteristiques.col.astype(str).describe()\n",
    "# 6 est la valeur la plus courante ( autre collision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remplacer nan par 6 :\n",
    "caracteristiques.loc[:,['col']]=caracteristiques.loc[:,['col']].replace(np.nan,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verification :\n",
    "print(len(caracteristiques.col.unique()))\n",
    "caracteristiques.col.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.col=caracteristiques.col.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adresse postale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(caracteristiques.adr.unique()))\n",
    "caracteristiques.adr.unique()\n",
    "# des adresse, donc pas de problémes ( sauf erreur de saisie. on va lire adresse par adresse bien sûr pour s'assurer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables numériques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# valeur nan par variable\n",
    "caracteristiques.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### lon/lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " lat et lon sont des variables numériques. Ils présentent plus de 50% de valeurs manquantes. On les garde comme ça pour l'instant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### com :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque dépertement contient des communes. Dans notre base les déparetements sont chiffrés entre 10 et 976. \n",
    "\n",
    "RQ :\n",
    "    - on peut aggréger nos données par département et étudier chaque département tout seul\n",
    "    - On peut encore ajouter de la granulité et aggréger par commune.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nan ==> le mode\n",
    "caracteristiques.com.astype(str).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caracteristiques.loc[:,['com']]=caracteristiques.loc[:,['com']].replace(np.nan,55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change type to object :\n",
    "caracteristiques.com=caracteristiques.com.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pas de valeur manquante pour cette variables\n",
    "print(\"il y a \",len(caracteristiques.dep.unique()),\"départements \")\n",
    "print(sorted(list(caracteristiques.dep.unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_values=list(caracteristiques.dep.astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set([dep for dep in dep_values if len(dep)==2])\n",
    "# on a bien toutes les valeurs correspondantes au dépertement de numéro entre 1 et 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(set([dep for dep in dep_values if len(dep)==3 and dep[-1]!='0' and dep!='202' and dep!='201'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 201 et 202 remplacent 2A et 2B ( voir lien www.francegene.com/rech-fr/dep-fr.php) : donc pour ces deux valeurs pas de soucis\n",
    "# mais les autres ils ne se terminent pas par 0 ?\n",
    "#  normalement il y a 95 departement  d'après le lien ci-dessous, donc 950 doit être la plus grande valeur.\n",
    "# A voir : soit se sont des nouveau dépertement ? sinon on va les remplacer par la plus courante valeurs.\n",
    "# en fait les autres numéros sont celles de la france d'outre mer. plus précisement :\n",
    "# 971 : num départemental de Guadeloupe\n",
    "# 972 : Maritinique\n",
    "# 973 : La Guyane \n",
    "# 974 : La Réunion\n",
    "# 976 : Mayotte\n",
    "# Donc c'est à nous de choisir : est ce que juste la france europééne ou aussi la france d'outre mer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change type to str\n",
    "caracteristiques.dep=caracteristiques.dep.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save table to csv file :\n",
    "caracteristiques.to_csv('caracteristiques_2005-2016.csv',index=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### la table lieux : \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par un graphiques montrant la distribution des valeurs manquantes dans cette table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install missingno # if you do not have this package try installing it using the given command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(lieux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous voyez il y a des variables qui ont beaucoup de valeurs manquantes( ceci sans compter les valeurs '0'), surtout : v1,v2, pr et pr1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant générer un heatmap de corrélation qui sert à montrer une distribution conditionnelle des valeurs manquantes entre les différentes variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.heatmap(lieux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une carte de chaleur de corrélation simple est montrée ci-dessus. Cette carte décrit le degré de relation de nullité entre les différentes variables. La plage de valeurs de cette corrélation de nullité va de -1 à 1 (-1 ≤ R ≤ 1). Les entités sans valeur manquante sont exclues du heatmap (par exemple: Num_Acc). Si la corrélation de nullité est très proche de zéro (-0,05 <R <0,05) (voie et catr), aucune valeur ne sera affichée. En outre, une corrélation de nullité positive parfaite (R = 1) indique que la première variable et la deuxième variable ont toutes deux des valeurs manquantes correspondantes (MAR), alors qu'une corrélation de nullité négative parfaite (R = -1) signifie que l'une des variables est manquante et la seconde n'est pas manquante (MNAR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par conséquent, vu que la majorité des valeurs de R sont importantes, on va considérer que les valeurs manquantes sont des MAR ( Missing At Random). Ceci a pour objectif de trouver la bonne méthode d'imputation des valeurs manquantes. Certe ici, la bonne méthode sera par exemple <b>MICE</b> (<b> Multiple Imputation by Chained Equations</b>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais cette méthode s'applique dans le cas où les variables sont numériques. Pour cela on se contente d'utiliser une méthode stationnaire d'imputation: imputation par le plus fréquent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'appliquer cette méthode, il faut nettoyer les cases avec les valeurs '0'. Ceci est important car ce n'est pas évident de remplacer les 0 par des nan simplement, puisque parfois '0' est une vraie valeurs à garder ( on verra des exemples de variables où ceci est vrai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### catégorie de route :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Les valeurs uniques existantes dans la variable catr sont : \\n\",lieux.catr.unique())\n",
    "# il y a nan dans cette série, donc nous devons les imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9 spécifie \"autre\"  type de route\n",
    "print(\"Il y a {} valeurs de 9 dans la variable catr\".format(lieux[lieux.catr==9].shape))  \n",
    "#taille valeur nan\n",
    "print(\"Il y a {} valeurs nan dans la variable catr \".format(lieux.catr.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remplacer nan par 9.( 9 toujours n'est pas connue donc implicitement c'est nan)\n",
    "lieux.loc[:,['catr']]=lieux.loc[:,['catr']].replace(np.nan,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### régime circulation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Les valeurs uniques existantes dans la variable circ sont : \",lieux.circ.unique())\n",
    "# il y a 0 et nan qui ne sont pas des vraies valeurs pour cette variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 : \",lieux[lieux.circ==0].shape[0])\n",
    "print(\"taille nan : \",lieux.circ.isnull().sum())\n",
    "# on remplace les 0 par nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on cherche le mode puis on impute nan et 0 par ce mode\n",
    "#print(float(lieux.circ.astype(str).describe().top))  # donne le mode de la série\n",
    "# top est 2( les accidents ont lieu beaucoup sur les  routes nationals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['circ']]=lieux.loc[:,['circ']].replace(np.nan,float(lieux.circ.astype(str).describe().top))\n",
    "lieux.loc[:,['circ']]=lieux.loc[:,['circ']].replace(0,float(lieux.circ.astype(str).describe().top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### voie réservée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Il y a {} comme valeurs uniques dans  la variable vosp\".format(lieux.vosp.unique()))\n",
    "# 0 et nan sont à corriger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\",lieux[lieux.vosp==0].shape)\n",
    "print(\"taille nan :\",lieux.vosp.isnull().sum())\n",
    "# on remarque que quasiment toute la colonne est en '0'.\n",
    "# on garde 0 ( car la variable 0 ici a pour signification : pas de voie réservèe)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['vosp']]=lieux.loc[:,['vosp']].replace(np.nan,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### profil de la route : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Les valeurs uniques pour la varible prof sont : \",lieux.prof.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\",lieux[lieux.prof==0].shape)\n",
    "print(\"taille nan :\",lieux.prof.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['prof']]=lieux.loc[:,['prof']].replace(0,float(lieux.prof.astype(str).describe().top))\n",
    "lieux.loc[:,['prof']]=lieux.loc[:,['prof']].replace(np.nan,float(lieux.prof.astype(str).describe().top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tracé en plan :\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"les valeurs uniques de la variable plan sont :\",lieux.plan.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\",lieux[lieux.plan==0].shape)\n",
    "print(\"taille nan :\",lieux.plan.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on remplace 0 par nan :\n",
    "lieux.plan.astype(str).describe()\n",
    "# top est 1 ( Partie rectiligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['plan']]=lieux.loc[:,['plan']].replace(0,lieux.plan.astype(str).describe().top)\n",
    "lieux.loc[:,['plan']]=lieux.loc[:,['plan']].replace(np.nan,lieux.plan.astype(str).describe().top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Etat de la surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Les valeurs uniques pour la variable surf sont : \",lieux.surf.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\",lieux[lieux.surf==0].shape)\n",
    "print(\"taille nan :\",lieux.surf.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on remplace par 9 ( car 9 veut dire autre autre)\n",
    "lieux.loc[:,['surf']]=lieux.loc[:,['surf']].replace(0,9)\n",
    "lieux.loc[:,['surf']]=lieux.loc[:,['surf']].replace(np.nan,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### situation de l'acccident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"les valeurs uniques pour la variable situ sont :\", lieux.situ.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\",lieux[lieux.situ==0].shape)\n",
    "print(\"taille nan :\",lieux.situ.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on remplace les 0 par nan :\n",
    "lieux.situ.astype(str).describe()\n",
    "#top est 1 ( sur chaussé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['situ']]=lieux.loc[:,['situ']].replace(0,float(lieux.situ.astype(str).describe().top))\n",
    "lieux.loc[:,['situ']]=lieux.loc[:,['situ']].replace(np.nan,float(lieux.situ.astype(str).describe().top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### point école :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on sait pas pour cette variables quelles sont les veleurs prises\n",
    "print(\"les valeurs uniques pour la variable env1 sont :\",lieux.env1.unique())\n",
    "# on laisse 0 comme valeur ( à voir ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### voie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"les valeurs uniques pour la variable voie sont :\", lieux.voie.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"total des nan dans voie :\",lieux.voie.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.voie.describe()\n",
    "# contient beaucoup de 0 alors que 0 n'est pas un numéro de route \n",
    "# on transforme 0 à nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['voie']]=lieux.loc[:,['voie']].replace(0,np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.loc[:,['voie']]=lieux.loc[:,['voie']].replace(np.nan,int(lieux.voie.describe().top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Indice numériques du numéro de la route : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Les valeurs uniques pour la variable v1 sont :\",lieux.v1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on remplace 0 par nan :\n",
    "lieux.loc[:,['v1']]=lieux.loc[:,['v1']].replace(0,int(lieux.v1.mean()))\n",
    "lieux.loc[:,['v1']]=lieux.loc[:,['v1']].replace(np.nan,int(lieux.v1.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lettre V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Les valeurs uniques pour la variable v2 sont :\", lieux.v2.unique())\n",
    "# il y a un probléme de encoding : /x04\n",
    "# il y des nan \n",
    "# il y des 0  \n",
    "# il faut comprendre la variable pour l'imputer.\n",
    "# C'est juste un indice, on peut éliminer cette variable et garder que voie et v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux=lieux.drop('v2',axis=1) # delete v2 from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### nombre total de voie de circulation  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sorted(set(list(lieux.nbv.unique()))))\n",
    "# il y a des nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable numérique==> on remplace par la moyenne\n",
    "lieux.loc[:,['nbv']]=lieux.loc[:,['nbv']].replace(np.nan,int(lieux.nbv.mean())) #  on prends la partie entiére"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Numéro PR :\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#de même variable numérique\n",
    "lieux.loc[:,['pr']]=lieux.loc[:,['pr']].replace(np.nan,(lieux.pr.mean())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance en mètres au PR :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable numérique\n",
    "\n",
    "lieux.loc[:,['pr1']]=lieux.loc[:,['pr1']].replace(np.nan,lieux.pr1.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Largeur du terre plein central TPC ( s il exist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable  numérique\n",
    "\n",
    "lieux.loc[:,['lartpc']]=lieux.loc[:,['lartpc']].replace(np.nan,lieux.lartpc.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### larrout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable numérique\n",
    "\n",
    "lieux.loc[:,['larrout']]=lieux.loc[:,['larrout']].replace(np.nan,lieux.larrout.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"les valeurs uniques dans la variable infra sont :\",lieux.infra.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lieux.infra.astype(str).describe()\n",
    "# on garde 0 comme étant une autre valeur.\n",
    "# on remplace nan par 0\n",
    "lieux.loc[:,['infra']]=lieux.loc[:,['infra']].replace(np.nan,float(lieux.infra.astype(str).describe().top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vient de finir le nettoyage de cette table. Il reste à donner le bon type pour chaque variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save table to csv file\n",
    "lieux.to_csv('lieux_2005-2016.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### la table usagers \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(usagers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### place : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.place.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\" , usagers[usagers.place==0].shape)\n",
    "print(\"taille nan :\" , usagers.place.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on les remplace par ? \n",
    "# ici les valeurs manquantes vont dependre de la nature du vehicule \n",
    "# donc pour chaque type de vehicule on va remplacer les valeurs manquantes par le mode.\n",
    "# donc on verra cette variable apès avoir fait la jointure entre les tables \n",
    "#sinon on peut simplifier et remplacer par le mode car dans tous les cas le mode sera la place 1 du conducteur\n",
    "usagers.place.astype(str).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.loc[:,['place']]=usagers.loc[:,['place']].replace(0,1)\n",
    "usagers.loc[:,['place']]=usagers.loc[:,['place']].replace(np.nan,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Catégorie d'usager :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.catu.unique()\n",
    "#c'est bon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gravité de l'accident​ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.grav.unique()\n",
    "#c'est bon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sexe de l'usager :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.sexe.unique()\n",
    "#c'est bon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Annee de naissance de l'usager'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.an_nais.dtype\n",
    "# pas de soucis ici sauf si il y une erreur dans la saisie  d'une data :\n",
    "print('min des dates :',usagers.an_nais.min())\n",
    "print('max des dates :',usagers.an_nais.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nan values :\n",
    "usagers.an_nais.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.loc[:,['an_nais']]=usagers.loc[:,['an_nais']].replace(np.nan,int(usagers.an_nais.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verification :\n",
    "print('min des dates :',usagers.an_nais.min())\n",
    "print('max des dates :',usagers.an_nais.max())\n",
    "# c'est bon la moyenne est comprise entre max et mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Motif du déplacement au moment de l’accident :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.trajet.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\" , usagers[usagers.trajet==0].shape)\n",
    "print(\"taille nan :\" , usagers.trajet.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on remplace par 9 :\n",
    "usagers.loc[:,['trajet']]=usagers.loc[:,['trajet']].replace(0,9)\n",
    "usagers.loc[:,['trajet']]=usagers.loc[:,['trajet']].replace(np.nan,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### secu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.secu.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secu_values=list(usagers.secu.astype(str).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on traite d'abord le len puis les nan :\n",
    "secu_values_set=[secu[:-2] for secu in secu_values if secu!='nan']\n",
    "# d'après le document chaque valeur est censée d'avoir deux chiffres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(set(secu_values_set))\n",
    "# il y le 0 et parfois on a qu'un seul chiffre ( ex 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.secu.astype(str).describe()\n",
    "#top 11 : ceinture exist et elle est utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remplacer nan et 0 par 11 :\n",
    "usagers.loc[:,['secu']]=usagers.loc[:,['secu']].replace(0,11)\n",
    "usagers.loc[:,['secu']]=usagers.loc[:,['secu']].replace(np.nan,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pour le reste : soit on simplifie et on remplace par 11\n",
    "# soit on compléte le chiffre par un deuxième qui soit le mode approprié au chiffre existant :\n",
    "# deuxième méthode est plus logique :\n",
    "from statistics import mode\n",
    "secu_1=[secu for secu in secu_values if '1' in secu]\n",
    "mode(secu_1)\n",
    "# donc on remplace par 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secu_2=[secu for secu in secu_values if '2' in secu]\n",
    "mode(secu_2)\n",
    "# on remplace par '21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secu_3=[secu for secu in secu_values if '3' in secu]\n",
    "mode(secu_3)\n",
    "# on remplace par '13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.loc[:,['secu']]=usagers.loc[:,['secu']].replace(1,11)\n",
    "usagers.loc[:,['secu']]=usagers.loc[:,['secu']].replace(2,21)\n",
    "usagers.loc[:,['secu']]=usagers.loc[:,['secu']].replace(3,13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Localisation du piéton :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.locp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\" , usagers[usagers.locp==0].shape)\n",
    "print(\"taille nan :\" , usagers.locp.isnull().sum())\n",
    "print(usagers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ici on va laisser 0 comme une autre valeur, il se peut qu'elle aie pour signification : 'il n'y a pas de piéton'\n",
    "# on remplace nan par 0 également \n",
    "usagers.loc[:,['locp']]=usagers.loc[:,['locp']].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Action du piéton :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.actp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nan ==> 0\n",
    "usagers.loc[:,['locp']]=usagers.loc[:,['locp']].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### etatp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.etatp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on laisse 0 comme une autre valeur qui va indiquer qu'il n'y avait pas de piéton\n",
    "# nan ==> 0\n",
    "usagers.loc[:,['etatp']]=usagers.loc[:,['etatp']].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vient de finir le nettoyage de cette table. Il reste à changer les types :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usagers.to_csv('usager_2005-2016.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "la table des vehicules\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(vehicules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sens de circulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.senc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"taille 0 :\" , vehicules[vehicules.senc==0].shape)\n",
    "print(\"taille nan :\" , vehicules.senc.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.senc.astype(str).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on remplace par une valeur au choix :  sens croissant \n",
    "vehicules.loc[:,['senc']]=vehicules.loc[:,['senc']].replace(np.nan,1)\n",
    "vehicules.loc[:,['senc']]=vehicules.loc[:,['senc']].replace(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Catégorie du véhicule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(vehicules.catv.unique()))\n",
    "vehicules.catv.unique()\n",
    "#Ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Obstacle mobile heurté :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.obsm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nan,0===>9\n",
    "vehicules.loc[:,['obsm']]=vehicules.loc[:,['obsm']].replace(np.nan,9)\n",
    "vehicules.loc[:,['obsm']]=vehicules.loc[:,['obsm']].replace(0,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Point de choc initial :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.choc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0, nan ==> mode :\n",
    "vehicules.choc.astype(str).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.loc[:,['choc']]=vehicules.loc[:,['choc']].replace(np.nan,1)\n",
    "vehicules.loc[:,['choc']]=vehicules.loc[:,['choc']].replace(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Nombre d’occupants dans le transport en commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.occutc.unique()\n",
    "# 0 pour dire que ce n'est pas transport commun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### manv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manv de type int\n",
    "vehicules.manv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.manv.astype(str).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.loc[:,['manv']]=vehicules.loc[:,['manv']].replace(np.nan,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicules.to_csv('vehicules_2005-2016.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fin nettoyage \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le but d'une bonne gestion de mémoire on va élimner toutes les variables, puis on récupére nos tables déjà nettoyées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La jointure des tables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par récupérer les données qu'on a déjà enregistré localement puis changer le type des variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlouriz\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#load caracteristiques and change types:\n",
    "caracteristiques=pd.read_csv('caracteristiques_2005-2016.csv', encoding='latin-1')\n",
    "caracteristiques.jour=caracteristiques.jour.astype(str)\n",
    "caracteristiques.mois=caracteristiques.mois.astype(str)\n",
    "caracteristiques.lum=caracteristiques.lum.astype(str)\n",
    "caracteristiques.loc[:,['agg']]=caracteristiques.loc[:,['agg']].astype(str)\n",
    "caracteristiques.int=caracteristiques.int.astype(str)\n",
    "caracteristiques.atm=caracteristiques.atm.astype(str)\n",
    "caracteristiques.col=caracteristiques.col.astype(str)\n",
    "\n",
    "\n",
    "#load lieux and change types:\n",
    "lieux=pd.read_csv('lieux_2005-2016.csv',encoding='latin-1')\n",
    "lieux.catr=lieux.catr.astype(str)\n",
    "lieux.circ=lieux.circ.astype(str)\n",
    "lieux.vosp=lieux.vosp.astype(str)\n",
    "lieux.prof=lieux.prof.astype(str)\n",
    "lieux.surf=lieux.surf.astype(str)\n",
    "lieux.infra=lieux.infra.astype(str)\n",
    "lieux.situ=lieux.situ.astype(str)\n",
    "\n",
    "# load vehicules and change types: \n",
    "vehicules=pd.read_csv('vehicules_2005-2016.csv',encoding='latin-1')\n",
    "vehicules.senc=vehicules.senc.astype(str)\n",
    "vehicules.catv=vehicules.catv.astype(str)\n",
    "vehicules.obs=vehicules.obs.astype(str)\n",
    "vehicules.obsm=vehicules.obsm.astype(str)\n",
    "vehicules.choc=vehicules.choc.astype(str)\n",
    "vehicules.manv=vehicules.manv.astype(str)\n",
    "\n",
    "# load usagers and change types:\n",
    "usagers=pd.read_csv('usager_2005-2016.csv',encoding='latin-1')\n",
    "usagers.place=usagers.place.astype(str)\n",
    "usagers.catu=usagers.catu.astype(str)\n",
    "usagers.grav=usagers.grav.astype(str)\n",
    "usagers.sexe=usagers.sexe.astype(str)\n",
    "usagers.trajet=usagers.trajet.astype(str)\n",
    "usagers.secu=usagers.secu.astype(str)\n",
    "usagers.locp=usagers.locp.astype(str)\n",
    "usagers.actp=usagers.actp.astype(str)\n",
    "usagers.etatp=usagers.etatp.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On merge caracteristiques et lieux puis on merge avec vehicules : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "carac_lieux_veh_merged=pd.merge(pd.merge(caracteristiques,lieux,on='Num_Acc'), vehicules,on='Num_Acc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on merge finalement avec usagers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_merged=pd.merge(carac_lieux_veh_merged,usagers,on=['Num_Acc','num_veh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data_merged into a csv file : \n",
    "data_merged.to_csv('data_merged.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Model de prédiction de la gravité des accidents avec toutes les données (2005-2016) :\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons effectuer un modèle de classification afin de prédire la gravité d'un accident en utilisant tout l'historique dont on dispose : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's first create an attribute age :\n",
    "data_merged.loc[:,['an']]=data_merged.loc[:,['an']]+2000\n",
    "data_merged['age']=pd.Series(data_merged.an.values - data_merged.an_nais.values)\n",
    "data_merged['age']=data_merged['age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_Acc</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>atm</th>\n",
       "      <th>col</th>\n",
       "      <th>...</th>\n",
       "      <th>catu</th>\n",
       "      <th>grav</th>\n",
       "      <th>sexe</th>\n",
       "      <th>trajet</th>\n",
       "      <th>secu</th>\n",
       "      <th>locp</th>\n",
       "      <th>actp</th>\n",
       "      <th>etatp</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Num_Acc    an mois jour   hrmn lum agg int  atm  col  ...   catu grav  \\\n",
       "0  200500000001  2005    1   12  19:00   3   2   1  1.0  3.0  ...      1    4   \n",
       "1  200500000001  2005    1   12  19:00   3   2   1  1.0  3.0  ...      1    3   \n",
       "2  200500000001  2005    1   12  19:00   3   2   1  1.0  3.0  ...      2    1   \n",
       "3  200500000001  2005    1   12  19:00   3   2   1  1.0  3.0  ...      2    1   \n",
       "4  200500000001  2005    1   12  19:00   3   2   1  1.0  3.0  ...      2    1   \n",
       "\n",
       "  sexe  trajet  secu  locp actp etatp  an_nais   age  \n",
       "0    1     1.0  11.0   0.0  0.0   0.0   1976.0  29.0  \n",
       "1    2     3.0  11.0   0.0  0.0   0.0   1968.0  37.0  \n",
       "2    1     9.0  11.0   0.0  0.0   0.0   1964.0  41.0  \n",
       "3    1     9.0  31.0   0.0  0.0   0.0   2004.0   1.0  \n",
       "4    1     9.0  11.0   0.0  0.0   0.0   1998.0   7.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Num_Acc', 'an', 'mois', 'jour', 'hrmn', 'lum', 'agg', 'int', 'atm',\n",
      "       'col', 'com', 'adr', 'gps', 'lat', 'long', 'dep', 'catr', 'voie', 'v1',\n",
      "       'circ', 'nbv', 'pr', 'pr1', 'vosp', 'prof', 'plan', 'lartpc', 'larrout',\n",
      "       'surf', 'infra', 'situ', 'env1', 'senc', 'catv', 'occutc', 'obs',\n",
      "       'obsm', 'choc', 'manv', 'num_veh', 'place', 'catu', 'grav', 'sexe',\n",
      "       'trajet', 'secu', 'locp', 'actp', 'etatp', 'an_nais', 'age'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_merged.columns)\n",
    "# columns used for classification : we detele all spatial features and also temporalle one \n",
    "col_used=[ 'lum', 'agg', 'int', 'atm','col',  'catr',\n",
    "       'circ', 'vosp', 'prof',\n",
    "        'surf', 'infra', 'situ', 'senc', 'catv','obsm', 'choc', 'manv', 'place', 'catu', 'grav',\n",
    "        'sexe', 'trajet', 'secu', 'locp', 'actp', 'etatp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have a windows machine and want to install xgboost, just follow instructiosn in this link :\n",
    "https://medium.com/@rakshithvasudev/how-i-installed-xgboost-after-a-lot-of-hassels-on-my-windows-machine-c53e972e801e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost and evaluations libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# data for training  and testing :\n",
    "data=data_merged.loc[:,col_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y=data.loc[:,data.columns!='grav'], data.loc[:,'grav']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrixxgboost_model=XGBClassifier()\n",
    "#fit model on training set\n",
    "xgboost_model.fit(X_train,y_train)\n",
    "#predict on test set :\n",
    "y_pred=xgboost_model.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(\"Confsuion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred))\n",
    "y_test.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1,Y_train1,Y_test1 = train_test_split(X, Y, test_size=0.33, random_state=99)\n",
    "#Without weather\n",
    "svc = SVC()\n",
    "svc.fit(X_train1, Y_train1)\n",
    "Y_pred = svc.predict(X_test1)\n",
    "acc_svc1 = round(svc.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_svc1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train1, Y_train1)\n",
    "Y_pred = knn.predict(X_test1)\n",
    "acc_knn1 = round(knn.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_knn1\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train1, Y_train1)\n",
    "Y_pred = logreg.predict(X_test1)\n",
    "acc_log1 = round(logreg.score(X_train1, Y_train1) * 100, 2)\n",
    "acc_log1\n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train1, Y_train1)\n",
    "Y_pred = gaussian.predict(X_test1)\n",
    "acc_gaussian1 = round(gaussian.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_gaussian1\n",
    "\n",
    "# Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train1, Y_train1)\n",
    "Y_pred = perceptron.predict(X_test1)\n",
    "acc_perceptron1 = round(perceptron.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_perceptron1\n",
    "\n",
    "# Linear SVC\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train1, Y_train1)\n",
    "Y_pred = linear_svc.predict(X_test1)\n",
    "acc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_linear_svc1\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train1, Y_train1)\n",
    "Y_pred = sgd.predict(X_test1)\n",
    "acc_sgd1 = round(sgd.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_sgd1\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train1, Y_train1)\n",
    "Y_pred = decision_tree.predict(X_test1)\n",
    "acc_decision_tree1 = round(decision_tree.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_decision_tree1\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train1, Y_train1)\n",
    "Y_pred = random_forest.predict(X_test1)\n",
    "random_forest.score(X_train1, Y_train1)\n",
    "acc_random_forest1 = round(random_forest.score(X_test1, Y_test1) * 100, 2)\n",
    "acc_random_forest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Machine Learning algorithm scores without weather related conditions\")\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc1, acc_knn1, acc_log1, \n",
    "              acc_random_forest1, acc_gaussian1, acc_perceptron1, \n",
    "              acc_sgd1, acc_linear_svc1, acc_decision_tree1]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-063f2fb33b8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Confusion matrix with random forest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 1)\n",
    "rf = RandomForestClassifier(random_state = 4)\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)\n",
    "print('Classification report: \\n',classification_report(y_test,y_pred))\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Jointure année par année"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x000002583E24FEF0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rlouriz\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlouriz\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "caracteristiques=pd.read_csv('caracteristiques_2005-2016.csv',encoding='latin-1')\n",
    "vehicules=pd.read_csv('vehicules_2005-2016.csv',encoding='latin-1')\n",
    "lieux=pd.read_csv('lieux_2005-2016.csv',encoding='latin-1')\n",
    "usagers=pd.read_csv('usager_2005-2016.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs=[] # a list that will contain data for each year\n",
    "col=[2005,2006,2007,2008,2010,2011,2012,2013,2014,2015,2016]\n",
    "for year in col :\n",
    "    c=caracteristiques.loc[caracteristiques['Num_Acc']//100000000==year] # c is a dataframe for year='year'\n",
    "    v=vehicules.loc[vehicules['Num_Acc']//100000000==year]\n",
    "    u=usagers.loc[usagers['Num_Acc']//100000000==year]\n",
    "    l=lieux.loc[lieux['Num_Acc']//100000000==year]\n",
    "    merge1=pd.merge(c,l,on='Num_Acc')\n",
    "    merge2=pd.merge(merge1,v,on='Num_Acc')\n",
    "    data=pd.merge(merge2,u,on=['Num_Acc','num_veh'])\n",
    "    dfs.append(data)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save all dataframes into a csvs files :\n",
    "for index, item in enumerate(dfs, start = 1):\n",
    "    item.to_csv('data'+str(index)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de prédiction de la gravité des accidents par année :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data=pd.DataFrame()\n",
    "for i in range(1,11):\n",
    "    df=pd.read_csv('data'+str(i)+'.csv',encoding='latin-1')\n",
    "    df= df.loc[(np.isfinite(df.long) & np.isfinite(df.lat)),:]\n",
    "    final_data=pd.concat([final_data,df])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of our data is :\",final_data.shape)\n",
    "print(\"We have {} columns in our data\".format(len(final_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save data as csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data.to_csv('data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset  # clean memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load csv file : datacleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data_cleaned.csv',encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=df.loc[:,col_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "train.to_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('data1.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's first create an attribute age :\n",
    "train.loc[:,['an']]=train.loc[:,['an']]+2000\n",
    "train['age']=pd.Series(train.an.values - train.an_nais.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.jour=train.jour.astype(str)\n",
    "train.lum=train.lum.astype(str)\n",
    "train.loc[:,['agg']]=train.loc[:,['agg']].astype(str)\n",
    "train.int=train.int.astype(str)\n",
    "train.atm=train.atm.astype(str)\n",
    "train.col=train.col.astype(str)\n",
    "\n",
    "train.catr=train.catr.astype(str)\n",
    "train.circ=train.circ.astype(str)\n",
    "train.prof=train.prof.astype(str)\n",
    "train.surf=train.surf.astype(str)\n",
    "train.infra=train.infra.astype(str)\n",
    "train.situ=train.situ.astype(str)\n",
    "\n",
    "train.senc=train.senc.astype(str)\n",
    "train.catv=train.catv.astype(str)\n",
    "train.obs=train.obs.astype(str)\n",
    "train.obsm=train.obsm.astype(str)\n",
    "train.choc=train.choc.astype(str)\n",
    "train.manv=train.manv.astype(str)\n",
    "\n",
    "\n",
    "train.place=train.place.astype(str)\n",
    "train.catu=train.catu.astype(str)\n",
    "train.grav=train.grav.astype(str)\n",
    "train.sexe=train.sexe.astype(str)\n",
    "train.trajet=train.trajet.astype(str)\n",
    "train.secu=train.secu.astype(str)\n",
    "train.locp=train.locp.astype(str)\n",
    "train.actp=train.actp.astype(str)\n",
    "train.etatp=train.etatp.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_used=[  'jour', 'lum', 'agg', 'int', 'atm', 'col',\n",
    "           'catr', 'voie', 'circ', 'nbv', 'pr', 'pr1', 'prof', 'plan', 'lartpc', 'larrout', 'surf', 'infra','obs',\n",
    "           'situ', 'senc', 'catv', 'occutc', 'obsm', 'choc', 'manv', 'place', 'catu', 'grav', 'sexe',\n",
    "           'trajet','secu', 'locp', 'etatp','age']\n",
    "col_used2=[   'lum',  'int', 'col',\n",
    "             'circ', 'nbv', 'surf', 'infra',\n",
    "           'situ', 'senc', 'catv', 'occutc',  'place', 'catu', 'grav', 'sexe',\n",
    "           'trajet','secu', 'locp','age']\n",
    "train=train.loc[:,col_used2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=train.grav.values\n",
    "X=train.loc[:,train.columns!='grav']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dummies :\n",
    "X=pd.get_dummies(X,prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confsuion matrix : \n",
      "  [[20866     1   435  2902]\n",
      " [  549    53   527   502]\n",
      " [ 3764    45  2115  6187]\n",
      " [ 8192    13  1424 11674]]\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.63      0.86      0.72     24204\n",
      "          2       0.47      0.03      0.06      1631\n",
      "          3       0.47      0.17      0.25     12111\n",
      "          4       0.55      0.55      0.55     21303\n",
      "\n",
      "avg / total       0.56      0.59      0.55     59249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "xgboost_model=XGBClassifier()\n",
    "#fit model on training set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)\n",
    "xgboost_model.fit(X_train,y_train)\n",
    "#predict on test set :\n",
    "y_pred=xgboost_model.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(\"Confsuion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(xgboost_model,out_file=\"tree.txt\",impurity = True,\n",
    "                            feature_names = list(X.columns),\n",
    "                            class_names = ['Indemne', 'Tué','BlesseHospitalisé','Blesséléger'],\n",
    "                            rounded = True,\n",
    "                            filled= True )tree.export_graphviz(dt,out_file=\"tree.txt\",impurity = True,\n",
    "                            feature_names = list(X.columns),\n",
    "                            class_names = ['Indemne', 'Tué','BlesseHospitalisé','Blesséléger'],\n",
    "                            rounded = True,\n",
    "                            filled= True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultats de xgboost avec getdummies est beaucoup plus meilleurs que sans getdummies.\n",
    "Ceci est important à comprendre: en effet toutes nos variables- à part qui sont numériques- sont nominales, donc le fait de laisser un codage en entier pour chaque variable ne donne pas des résultats optimale vu que l'algorithme va les considérer comme étant des variables ordianales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 24204, 2: 1631, 3: 12111, 4: 21303})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import  Counter\n",
    "Counter(list(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[19852    45  1148  3159]\n",
      " [  312   154   769   396]\n",
      " [ 2460   222  4706  4723]\n",
      " [ 5964    90  3935 11314]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.69      0.82      0.75     24204\n",
      "          2       0.30      0.09      0.14      1631\n",
      "          3       0.45      0.39      0.42     12111\n",
      "          4       0.58      0.53      0.55     21303\n",
      "\n",
      "avg / total       0.59      0.61      0.60     59249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix with random forest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#x,y = final_data.loc[:,final_data.columns != 'grav'], final_data.loc[:,'grav']\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = 1)\n",
    "rf = RandomForestClassifier(random_state = 4)\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)\n",
    "print('Classification report: \\n',classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07491471,  0.02161795,  0.00995786,  0.02140831,  0.01737334,\n",
       "        0.03932014,  0.02373142,  0.03947745,  0.01627951,  0.03085286,\n",
       "        0.04362372,  0.03262579,  0.0163669 ,  0.01592698,  0.01728349,\n",
       "        0.06826311,  0.0154436 ,  0.01175445,  0.0297301 ,  0.00861901,\n",
       "        0.00037745,  0.07667873,  0.00130716,  0.03048254,  0.03838609,\n",
       "        0.04603939,  0.01636648,  0.03073791,  0.01814418,  0.02773542,\n",
       "        0.03824001,  0.00756619,  0.01940826,  0.09395949])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_\n",
    "#l'age est important pour déterminer la gravité des accidents  (résultats du feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[20938     1   391  2874]\n",
      " [  614    45   440   532]\n",
      " [ 4034    37  1723  6317]\n",
      " [ 8598     8  1315 11382]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.61      0.87      0.72     24204\n",
      "          2       0.49      0.03      0.05      1631\n",
      "          3       0.45      0.14      0.22     12111\n",
      "          4       0.54      0.53      0.54     21303\n",
      "\n",
      "avg / total       0.55      0.58      0.53     59249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = 1)\n",
    "dt = DecisionTreeClassifier(random_state = 4,max_depth=6)\n",
    "dt.fit(x_train,y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)\n",
    "print('Classification report: \\n',classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import  GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5815099061824126\n",
      "{'max_depth': 10, 'min_samples_split': 15}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import  GridSearchCV\n",
    "dt = DecisionTreeClassifier(random_state = 4)\n",
    "grid_values = {'max_depth': [3,4,5,6,7,8,9,10,],'min_samples_split':[5,10,15]}\n",
    "#metric to optimize over grid parameters: Recall\n",
    "grid_clf_recall = GridSearchCV(dt, param_grid = grid_values,scoring='accuracy')\n",
    "grid_clf_recall.fit(x_train, y_train)\n",
    "\n",
    "print(grid_clf_recall.best_score_)\n",
    "print(grid_clf_recall.best_params_)\n",
    "#y_decision_fn_scores_recall = grid_clf_recall.predict(X_test) \n",
    "\n",
    "#print('Test set Recall: ', recall_score(y_test, y_decision_fn_scores_recall))\n",
    "#print('Grid best parameter (max. Recall): ', grid_clf_recall.best_params_)\n",
    "#print('Grid best score (Recall): ', grid_clf_recall.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.581815726848\n"
     ]
    }
   ],
   "source": [
    "dt=grid_clf_recall.best_estimator_\n",
    "dt.fit(x_train,y_train)\n",
    "print('test accuracy:',dt.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "with open(\"tree1.dot\",\"w\") as f :\n",
    "    tree.export_graphviz(dt,out_file=f,impurity = True,\n",
    "                            feature_names = list(X.columns),\n",
    "                            class_names = ['Indemne', 'Tué','BlesseHospitalisé','Blesséléger'],\n",
    "                            rounded = True,\n",
    "                            filled= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(dt,out_file=\"tree.txt\",impurity = True,\n",
    "                            feature_names = list(X.columns),\n",
    "                            class_names = ['Indemne', 'Tué','BlesseHospitalisé','Blesséléger'],\n",
    "                            rounded = True,\n",
    "                            filled= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
